{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example demonstrating enhanced training with different cell types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nfrom ncps.mlx import CTRNN, CTGRU, ELTC\nfrom ncps.mlx.training import EnhancedLTCTrainer, TrainingConfig\n\n# Generate sequence data\nN = 1000  # sequence length\nin_features = 2  # input dimension\nout_features = 1  # output dimension\nbatch_size = 1  # using batch_size=1 for this example\n\n# Generate input sequence: [batch_size, seq_length, features]\ndata_x = mx.stack([\n    mx.sin(mx.linspace(0, 3 * mx.pi, N)), \n    mx.cos(mx.linspace(0, 3 * mx.pi, N))\n], axis=1)  # Shape: [N, 2]\ndata_x = mx.expand_dims(data_x, axis=0)  # Shape: [1, N, 2]\n\n# Generate target sequence: [batch_size, seq_length, output_dim]\ndata_y = mx.sin(mx.linspace(0, 6 * mx.pi, N))  # Shape: [N]\ndata_y = mx.reshape(data_y, (1, N, out_features))  # Shape: [1, N, 1]\n\n# Generate time delta information: [batch_size, seq_length]\n# Use moderate time steps for stability\ntime_delta = mx.clip(mx.random.uniform(low=0.05, high=0.15, shape=(batch_size, N)), 0.05, 0.15)\n\n# Initialize with explicit random seed for reproducibility\nmx.random.seed(42)\n\n# Training configuration with more conservative hyperparameters\nconfig = TrainingConfig(\n    target_accuracy=95.0,\n    max_epochs=2000,  # Increased max epochs for slower learning\n    patience=200,     # Increased patience\n    weight_clip=0.1,  # Reduced weight clip\n    bias_clip=0.05,   # Reduced bias clip\n    max_grad_norm=0.5,  # Reduced grad norm\n    learning_rate=0.001,  # Reduced learning rate\n    min_learning_rate=0.0001,  # Lower min learning rate\n    warmup_epochs=500,  # Longer warmup period\n    noise_scale=0.01,  # Reduced noise scale\n    noise_decay=0.9999,  # Slower noise decay\n    momentum=0.99,  # Higher momentum\n    grad_momentum=0.1  # Low gradient momentum\n)\n\n# Create trainer\ntrainer = EnhancedLTCTrainer(config)\n\n# Define model configurations\nmodel_configs = [\n    (\n        \"CTRNN\",\n        CTRNN(\n            units=32,  # Moderate number of units\n            activation=\"tanh\",  # Using string activation name\n            cell_clip=0.5  # Moderate cell clip\n        )\n    ),\n    (\n        \"CTGRU\",\n        CTGRU(\n            units=32,  # Moderate number of units\n            cell_clip=0.5  # Moderate cell clip\n        )\n    ),\n    (\n        \"ELTC\",\n        ELTC(\n            input_size=in_features,\n            hidden_size=32,\n            ode_unfolds=6,\n            activation=\"tanh\",\n            cell_clip=0.5,  # Moderate cell clip\n            return_sequences=True\n        )\n    )\n]\n\n# Train each model configuration\nfor name, model in model_configs:\n    print(f\"\\nTraining {name}\")\n    \n    # Initialize model by doing a forward pass\n    dummy_input = mx.zeros((batch_size, 1, in_features))\n    _ = model(dummy_input)\n    \n    # Print model parameters\n    print(\"Model parameters:\")\n    def print_params(params, prefix=\"\"):\n        for name, param in params.items():\n            if isinstance(param, dict):\n                print(f\"{prefix}{name}:\")\n                print_params(param, prefix + \"  \")\n            elif isinstance(param, list):\n                print(f\"{prefix}{name}: list of {len(param)} items\")\n                for i, item in enumerate(param):\n                    if hasattr(item, 'shape'):\n                        print(f\"{prefix}  [{i}]: shape={item.shape}\")\n                    else:\n                        print(f\"{prefix}  [{i}]: {type(item)}\")\n            elif hasattr(param, 'shape'):\n                print(f\"{prefix}{name}: shape={param.shape}\")\n            else:\n                print(f\"{prefix}{name}: {type(param)}\")\n    \n    print_params(model.parameters())\n    \n    # Train model\n    history = trainer.train_with_accuracy_target(\n        model=model,\n        data_x=data_x,\n        data_y=data_y,\n        verbose=True\n    )\n    \n    # Print results\n    print(f\"\\n{name} Results:\")\n    print(f\"Best accuracy: {history['best_accuracy']:.2f}%\")\n    print(f\"Best epoch: {history['best_epoch']}\")\n    print(f\"Converged: {history['converged']}\")\n    print(f\"Final loss: {history['loss'][-1]:.6f}\")\n    \n    # Evaluate final predictions\n    predictions = model(data_x, time_delta=time_delta)\n    final_loss = mx.mean((predictions - data_y) ** 2)\n    mx.eval(final_loss)  # Force evaluation\n    print(f\"Evaluation loss: {final_loss.item():.6f}\")\n\nprint(\"\\nTraining complete.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}