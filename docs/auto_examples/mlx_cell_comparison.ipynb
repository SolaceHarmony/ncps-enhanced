{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example comparing different MLX cell implementations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nfrom ncps.mlx import CTRNN, CTGRU, ELTC\nfrom ncps.mlx.wirings import FullyConnected\n\n# Generate sequence data\nN = 1000  # sequence length\nin_features = 2  # input dimension\nout_features = 1  # output dimension\nbatch_size = 1  # using batch_size=1 for this example\n\n# Generate input sequence: [batch_size, seq_length, features]\ndata_x = mx.stack([\n    mx.sin(mx.linspace(0, 3 * mx.pi, N)), \n    mx.cos(mx.linspace(0, 3 * mx.pi, N))\n], axis=1)  # Shape: [N, 2]\ndata_x = mx.expand_dims(data_x, axis=0)  # Shape: [1, N, 2]\n\n# Generate target sequence: [batch_size, seq_length, output_dim]\ndata_y = mx.sin(mx.linspace(0, 6 * mx.pi, N))  # Shape: [N]\ndata_y = mx.reshape(data_y, (1, N, out_features))  # Shape: [1, N, 1]\n\n# Generate time delta information: [batch_size, seq_length]\ntime_delta = mx.clip(mx.random.uniform(low=0.9, high=1.1, shape=(batch_size, N)), 0.9, 1.1)\n\n# Initialize with explicit random seed for reproducibility\nmx.random.seed(42)\n\n# Create wiring\nwiring = FullyConnected(units=8, output_dim=out_features)\nwiring.build(in_features)\n\n# Define model configurations\nmodel_configs = [\n    (\n        \"CTRNN\",\n        CTRNN(\n            units=8,\n            activation=mx.tanh,\n            cell_clip=1.0\n        )\n    ),\n    (\n        \"CTGRU\",\n        CTGRU(\n            units=8,\n            cell_clip=1.0\n        )\n    ),\n    (\n        \"ELTC\",\n        ELTC(\n            input_size=in_features,\n            hidden_size=8,\n            solver=\"rk4\",\n            ode_unfolds=6\n        )\n    )\n]\n\n# Define loss function\ndef mse_loss(y_pred, y_true):\n    return mx.mean((y_pred - y_true) ** 2)\n\n# Training configuration\ntraining_config = {\n    'num_epochs': 100,\n    'learning_rate': 0.001,\n    'max_grad_norm': 0.1,\n    'max_grad_value': 1.0,\n}\n\n# Train each model configuration\nfor name, model in model_configs:\n    print(f\"\\nTraining {name}\")\n    \n    # Initialize optimizer\n    optimizer = optim.Adam(\n        learning_rate=training_config['learning_rate'],\n        betas=[0.9, 0.999],\n        eps=1e-8\n    )\n    \n    # Define loss function that will be used for gradient computation\n    def loss_fn(params):\n        model.update(params)\n        pred = model(data_x, time_delta=time_delta)\n        return mse_loss(pred, data_y)\n    \n    # Training loop\n    best_loss = float('inf')\n    patience = 10\n    patience_counter = 0\n    \n    for epoch in range(training_config['num_epochs']):\n        # Compute loss and gradients\n        loss, grads = mx.value_and_grad(loss_fn)(model.trainable_parameters())\n        \n        # Clip gradients\n        grad_norm = mx.sqrt(sum(mx.sum(g * g) for _, g in mx.tree_flatten(grads)))\n        if grad_norm > training_config['max_grad_norm']:\n            scale = training_config['max_grad_norm'] / (grad_norm + 1e-6)\n            grads = mx.tree_map(lambda g: g * scale, grads)\n        \n        # Update model parameters\n        optimizer.update(model, grads)\n        \n        if mx.isnan(loss):\n            print(f\"Training {name} failed at epoch {epoch} with NaN loss.\")\n            break\n        \n        # Early stopping check\n        if loss < best_loss:\n            best_loss = loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= patience:\n            print(f\"Early stopping triggered at epoch {epoch}\")\n            break\n        \n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch + 1}, Loss: {loss.item():.6f}\")\n            \n        # Evaluate predictions periodically\n        if (epoch + 1) % 50 == 0:\n            pred = model(data_x, time_delta=time_delta)\n            eval_loss = mse_loss(pred, data_y)\n            print(f\"Evaluation Loss: {eval_loss.item():.6f}\")\n\nprint(\"\\nTraining complete. Final losses:\")\nfor name, model in model_configs:\n    pred = model(data_x, time_delta=time_delta)\n    final_loss = mse_loss(pred, data_y)\n    print(f\"{name}: {final_loss.item():.6f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}