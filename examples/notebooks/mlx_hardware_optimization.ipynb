{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Silicon Optimization Guide\n",
    "\n",
    "This notebook demonstrates how to optimize Neural Circuit Policies for Apple Silicon processors:\n",
    "\n",
    "- Neural Engine Optimization\n",
    "- Memory Management\n",
    "- Performance Profiling\n",
    "- Hardware-Specific Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from ncps.mlx import CfC, CfCCell, LTC, LTCCell\n",
    "from ncps.wirings import Random, NCP, AutoNCP\n",
    "from ncps.mlx.advanced_profiling import MLXProfiler, quick_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Engine Optimization\n",
    "\n",
    "MLX automatically leverages the Neural Engine for supported operations. Here's how to optimize for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def optimize_for_neural_engine(sizes=[32, 64, 128]):\n",
    "    \"\"\"Compare configurations optimized for Neural Engine.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Test different model configurations\n",
    "    for hidden_size in sizes:\n",
    "        # Create wiring with power-of-2 sizes for efficiency\n",
    "        wiring = AutoNCP(\n",
    "            units=hidden_size,\n",
    "            output_size=hidden_size // 4,\n",
    "            sparsity_level=0.5\n",
    "        )\n",
    "        \n",
    "        # Create model with Neural Engine-friendly configuration\n",
    "        model = CfC(\n",
    "            cell=CfCCell(\n",
    "                wiring=wiring,\n",
    "                activation=\"tanh\",\n",
    "                backbone_units=[hidden_size, hidden_size],  # Power of 2 sizes\n",
    "                backbone_layers=2\n",
    "            ),\n",
    "            return_sequences=True\n",
    "        )\n",
    "        \n",
    "        # Test different batch sizes\n",
    "        batch_sizes = [16, 32, 64, 128]\n",
    "        for batch_size in batch_sizes:\n",
    "            profiler = MLXProfiler(model)\n",
    "            \n",
    "            # Profile with and without compilation\n",
    "            for compiled in [False, True]:\n",
    "                if compiled:\n",
    "                    # Compile for static shapes\n",
    "                    @mx.compile(static_argnums=(1,))\n",
    "                    def forward(x, training=True):\n",
    "                        return model(x, training=training)\n",
    "                else:\n",
    "                    forward = lambda x, training: model(x, training=training)\n",
    "                \n",
    "                stats = profiler.profile_compute(\n",
    "                    batch_size=batch_size,\n",
    "                    seq_length=16,  # Power of 2\n",
    "                    num_runs=50,\n",
    "                    forward_fn=forward\n",
    "                )\n",
    "                \n",
    "                memory_stats = profiler.profile_memory(\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'size': hidden_size,\n",
    "                    'batch_size': batch_size,\n",
    "                    'compiled': compiled,\n",
    "                    'tflops': stats['tflops'],\n",
    "                    'memory': memory_stats['peak_usage'],\n",
    "                    'time': stats['time_mean']\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run Neural Engine optimization\n",
    "ne_results = optimize_for_neural_engine()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot TFLOPS\n",
    "plt.subplot(131)\n",
    "for compiled in [False, True]:\n",
    "    data = [r for r in ne_results if r['size'] == 64 and r['compiled'] == compiled]\n",
    "    plt.plot(\n",
    "        [d['batch_size'] for d in data],\n",
    "        [d['tflops'] for d in data],\n",
    "        marker='o',\n",
    "        label=f'{\\'Compiled\\' if compiled else \\'Uncompiled\\'}'\n",
    "    )\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('TFLOPS')\n",
    "plt.title('Neural Engine Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Memory Usage\n",
    "plt.subplot(132)\n",
    "sizes = [32, 64, 128]\n",
    "for size in sizes:\n",
    "    data = [r for r in ne_results if r['size'] == size and not r['compiled']]\n",
    "    plt.plot(\n",
    "        [d['batch_size'] for d in data],\n",
    "        [d['memory'] for d in data],\n",
    "        marker='o',\n",
    "        label=f'Size {size}'\n",
    "    )\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.title('Memory Usage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Execution Time\n",
    "plt.subplot(133)\n",
    "for compiled in [False, True]:\n",
    "    data = [r for r in ne_results if r['size'] == 64 and r['compiled'] == compiled]\n",
    "    plt.plot(\n",
    "        [d['batch_size'] for d in data],\n",
    "        [d['time']*1000 for d in data],\n",
    "        marker='o',\n",
    "        label=f'{\\'Compiled\\' if compiled else \\'Uncompiled\\'}'\n",
    "    )\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Execution Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Management\n",
    "\n",
    "MLX's unified memory architecture requires careful management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MemoryOptimizedTrainer:\n",
    "    def __init__(self, model, learning_rate=0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = nn.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "    @mx.compile(static_argnums=(1,))\n",
    "    def train_step(self, training=True):\n",
    "        def loss_fn(model, x, y):\n",
    "            pred = model(x, training=training)\n",
    "            return mx.mean((pred - y) ** 2)\n",
    "        return mx.value_and_grad(self.model, loss_fn)\n",
    "    \n",
    "    def train(self, X, y, batch_size=32, epochs=10):\n",
    "        history = {'loss': [], 'memory': [], 'time': []}\n",
    "        n_batches = len(X) // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Shuffle data\n",
    "            indices = mx.random.permutation(len(X))\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                batch_x = X[start_idx:end_idx]\n",
    "                batch_y = y[start_idx:end_idx]\n",
    "                \n",
    "                # Compute loss and gradients\n",
    "                loss, grads = self.train_step()(self.model, batch_x, batch_y)\n",
    "                \n",
    "                # Update weights\n",
    "                self.optimizer.update(self.model, grads)\n",
    "                \n",
    "                epoch_loss += float(loss)\n",
    "            \n",
    "            # Record metrics\n",
    "            history['loss'].append(epoch_loss / n_batches)\n",
    "            history['time'].append(time() - epoch_start)\n",
    "            \n",
    "            # Profile memory\n",
    "            profiler = MLXProfiler(self.model)\n",
    "            memory_stats = profiler.profile_memory(batch_size=batch_size)\n",
    "            history['memory'].append(memory_stats['peak_usage'])\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {history['loss'][-1]:.4f}\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "# Generate sample data\n",
    "X = mx.random.normal((1000, 16, 8))\n",
    "y = mx.random.normal((1000, 16, 1))\n",
    "\n",
    "# Create models with different configurations\n",
    "configs = [\n",
    "    ('Small', 32, 32),\n",
    "    ('Medium', 64, 64),\n",
    "    ('Large', 128, 128)\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for name, hidden_size, batch_size in configs:\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    \n",
    "    # Create model\n",
    "    wiring = AutoNCP(units=hidden_size, output_size=1)\n",
    "    model = CfC(\n",
    "        cell=CfCCell(\n",
    "            wiring=wiring,\n",
    "            backbone_units=[hidden_size],\n",
    "            backbone_layers=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer = MemoryOptimizedTrainer(model)\n",
    "    results[name] = trainer.train(X, y, batch_size=batch_size, epochs=5)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.subplot(131)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['loss'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Memory Usage\n",
    "plt.subplot(132)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['memory'], marker='o', label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.title('Peak Memory Usage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Training Time\n",
    "plt.subplot(133)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['time'], marker='o', label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Training Time per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware-Specific Recommendations\n",
    "\n",
    "Based on our experiments:\n",
    "\n",
    "1. **Neural Engine Optimization**\n",
    "   - Use power-of-2 sizes for tensors\n",
    "   - Enable MLX compilation\n",
    "   - Batch sizes: 32-128 work best\n",
    "   - Use static shapes when possible\n",
    "\n",
    "2. **Memory Management**\n",
    "   - Leverage unified memory\n",
    "   - Clear unused variables\n",
    "   - Use appropriate batch sizes\n",
    "   - Monitor memory usage\n",
    "\n",
    "3. **Performance Tips**\n",
    "   - Profile your specific device\n",
    "   - Use MLX's lazy evaluation\n",
    "   - Enable operator fusion\n",
    "   - Monitor hardware utilization\n",
    "\n",
    "4. **Device-Specific Settings**\n",
    "   - M1: 32-64 batch size\n",
    "   - M1 Pro/Max: 64-128 batch size\n",
    "   - M1 Ultra: 128-256 batch size\n",
    "   - Adjust based on your model size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
