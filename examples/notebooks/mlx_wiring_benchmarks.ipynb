{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Circuit Policy Benchmarks\n",
    "\n",
    "This notebook provides comprehensive benchmarks for different wiring patterns across common tasks:\n",
    "- Sequence Prediction\n",
    "- Time Series Classification\n",
    "- Control Tasks\n",
    "- Real-time Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from ncps.mlx import CfC, LTC\n",
    "from ncps.mlx.wirings import Random, NCP, AutoNCP\n",
    "from ncps.mlx.advanced_profiling import MLXProfiler, quick_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence Prediction\n",
    "\n",
    "Benchmark sequence prediction performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_sequence_data(n_samples=1000, seq_length=50):\n",
    "    \"\"\"Generate sequence prediction data.\"\"\"\n",
    "    X = np.zeros((n_samples, seq_length, 1))\n",
    "    y = np.zeros((n_samples, seq_length, 1))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Generate sinusoidal sequence\n",
    "        t = np.linspace(0, 4*np.pi, seq_length)\n",
    "        freq = 1.0 + 0.1 * np.random.randn()\n",
    "        phase = 2 * np.pi * np.random.rand()\n",
    "        X[i, :, 0] = np.sin(freq * t + phase)\n",
    "        y[i, :, 0] = np.cos(freq * t + phase)  # Predict derivative\n",
    "    \n",
    "    return mx.array(X), mx.array(y)\n",
    "\n",
    "def benchmark_sequence_prediction():\n",
    "    \"\"\"Benchmark sequence prediction task.\"\"\"\n",
    "    # Generate data\n",
    "    X_train, y_train = generate_sequence_data()\n",
    "    X_test, y_test = generate_sequence_data(n_samples=100)\n",
    "    \n",
    "    # Define models to test\n",
    "    models = {\n",
    "        'Random Dense': CfC(Random(units=100, sparsity_level=0.2)),\n",
    "        'Random Sparse': CfC(Random(units=100, sparsity_level=0.8)),\n",
    "        'NCP': CfC(NCP(\n",
    "            inter_neurons=50,\n",
    "            command_neurons=30,\n",
    "            motor_neurons=20,\n",
    "            sensory_fanout=5,\n",
    "            inter_fanout=5,\n",
    "            recurrent_command_synapses=10,\n",
    "            motor_fanin=5\n",
    "        )),\n",
    "        'AutoNCP': CfC(AutoNCP(units=100, output_size=1))\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        optimizer = nn.Adam(learning_rate=0.001)\n",
    "        train_losses = []\n",
    "        train_time = time()\n",
    "        \n",
    "        for epoch in range(100):\n",
    "            def loss_fn(model, x, y):\n",
    "                pred = model(x)\n",
    "                return mx.mean((pred - y) ** 2)\n",
    "            \n",
    "            loss, grads = mx.value_and_grad(model, loss_fn)(model, X_train, y_train)\n",
    "            optimizer.update(model, grads)\n",
    "            train_losses.append(float(loss))\n",
    "        \n",
    "        train_time = time() - train_time\n",
    "        \n",
    "        # Evaluate\n",
    "        pred = model(X_test)\n",
    "        test_loss = float(mx.mean((pred - y_test) ** 2))\n",
    "        \n",
    "        # Profile\n",
    "        stats = quick_profile(model)\n",
    "        \n",
    "        results[name] = {\n",
    "            'train_time': train_time,\n",
    "            'train_loss': train_losses,\n",
    "            'test_loss': test_loss,\n",
    "            'tflops': stats['compute']['tflops'],\n",
    "            'memory': stats['memory']['peak_usage']\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "sequence_results = benchmark_sequence_prediction()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "for name, result in sequence_results.items():\n",
    "    plt.plot(result['train_loss'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.bar(sequence_results.keys(),\n",
    "        [r['train_time'] for r in sequence_results.values()])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Training Time')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.bar(sequence_results.keys(),\n",
    "        [r['tflops'] for r in sequence_results.values()])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('TFLOPS')\n",
    "plt.title('Compute Efficiency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Classification\n",
    "\n",
    "Benchmark classification performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_classification_data(n_samples=1000, seq_length=50, n_classes=5):\n",
    "    \"\"\"Generate time series classification data.\"\"\"\n",
    "    X = np.zeros((n_samples, seq_length, 1))\n",
    "    y = np.zeros((n_samples, n_classes))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Generate pattern based on class\n",
    "        class_id = np.random.randint(n_classes)\n",
    "        y[i, class_id] = 1\n",
    "        \n",
    "        t = np.linspace(0, 4*np.pi, seq_length)\n",
    "        if class_id == 0:\n",
    "            # Sine wave\n",
    "            X[i, :, 0] = np.sin(t)\n",
    "        elif class_id == 1:\n",
    "            # Square wave\n",
    "            X[i, :, 0] = np.sign(np.sin(t))\n",
    "        elif class_id == 2:\n",
    "            # Sawtooth\n",
    "            X[i, :, 0] = t % (2*np.pi) - np.pi\n",
    "        else:\n",
    "            # Random patterns\n",
    "            X[i, :, 0] = np.cumsum(np.random.randn(seq_length)) / np.sqrt(seq_length)\n",
    "    \n",
    "    return mx.array(X), mx.array(y)\n",
    "\n",
    "def benchmark_classification():\n",
    "    \"\"\"Benchmark classification task.\"\"\"\n",
    "    # Generate data\n",
    "    X_train, y_train = generate_classification_data()\n",
    "    X_test, y_test = generate_classification_data(n_samples=100)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Random Dense': CfC(Random(units=100, sparsity_level=0.2)),\n",
    "        'Random Sparse': CfC(Random(units=100, sparsity_level=0.8)),\n",
    "        'NCP': CfC(NCP(\n",
    "            inter_neurons=50,\n",
    "            command_neurons=30,\n",
    "            motor_neurons=5,\n",
    "            sensory_fanout=5,\n",
    "            inter_fanout=5,\n",
    "            recurrent_command_synapses=10,\n",
    "            motor_fanin=5\n",
    "        )),\n",
    "        'AutoNCP': CfC(AutoNCP(units=100, output_size=5))\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        optimizer = nn.Adam(learning_rate=0.001)\n",
    "        train_losses = []\n",
    "        train_time = time()\n",
    "        \n",
    "        for epoch in range(100):\n",
    "            def loss_fn(model, x, y):\n",
    "                logits = model(x)[:, -1]  # Use final output\n",
    "                return mx.mean((logits - y) ** 2)\n",
    "            \n",
    "            loss, grads = mx.value_and_grad(model, loss_fn)(model, X_train, y_train)\n",
    "            optimizer.update(model, grads)\n",
    "            train_losses.append(float(loss))\n",
    "        \n",
    "        train_time = time() - train_time\n",
    "        \n",
    "        # Evaluate\n",
    "        pred = model(X_test)[:, -1]\n",
    "        accuracy = float(mx.mean(mx.argmax(pred, axis=1) == mx.argmax(y_test, axis=1)))\n",
    "        \n",
    "        # Profile\n",
    "        stats = quick_profile(model)\n",
    "        \n",
    "        results[name] = {\n",
    "            'train_time': train_time,\n",
    "            'train_loss': train_losses,\n",
    "            'accuracy': accuracy,\n",
    "            'tflops': stats['compute']['tflops'],\n",
    "            'memory': stats['memory']['peak_usage']\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "classification_results = benchmark_classification()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "for name, result in classification_results.items():\n",
    "    plt.plot(result['train_loss'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.bar(classification_results.keys(),\n",
    "        [r['accuracy'] for r in classification_results.values()])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classification Accuracy')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.bar(classification_results.keys(),\n",
    "        [r['memory'] for r in classification_results.values()])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.title('Memory Usage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Control Tasks\n",
    "\n",
    "Benchmark control performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_control_data(n_samples=1000, seq_length=50):\n",
    "    \"\"\"Generate control task data.\"\"\"\n",
    "    X = np.zeros((n_samples, seq_length, 4))  # State: [pos_x, pos_y, vel_x, vel_y]\n",
    "    y = np.zeros((n_samples, seq_length, 2))  # Control: [force_x, force_y]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Generate circular trajectory\n",
    "        t = np.linspace(0, 2*np.pi, seq_length)\n",
    "        radius = 1.0 + 0.1 * np.random.randn()\n",
    "        phase = 2 * np.pi * np.random.rand()\n",
    "        \n",
    "        # Position\n",
    "        X[i, :, 0] = radius * np.cos(t + phase)\n",
    "        X[i, :, 1] = radius * np.sin(t + phase)\n",
    "        \n",
    "        # Velocity\n",
    "        X[i, :, 2] = -radius * np.sin(t + phase)\n",
    "        X[i, :, 3] = radius * np.cos(t + phase)\n",
    "        \n",
    "        # Optimal control (acceleration)\n",
    "        y[i, :, 0] = -radius * np.cos(t + phase)\n",
    "        y[i, :, 1] = -radius * np.sin(t + phase)\n",
    "    \n",
    "    return mx.array(X), mx.array(y)\n",
    "\n",
    "def benchmark_control():\n",
    "    \"\"\"Benchmark control task.\"\"\"\n",
    "    # Generate data\n",
    "    X_train, y_train = generate_control_data()\n",
    "    X_test, y_test = generate_control_data(n_samples=100)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Random Dense': CfC(Random(units=100, sparsity_level=0.2)),\n",
    "        'Random Sparse': CfC(Random(units=100, sparsity_level=0.8)),\n",
    "        'NCP': CfC(NCP(\n",
    "            inter_neurons=50,\n",
    "            command_neurons=30,\n",
    "            motor_neurons=2,\n",
    "            sensory_fanout=5,\n",
    "            inter_fanout=5,\n",
    "            recurrent_command_synapses=10,\n",
    "            motor_fanin=5\n",
    "        )),\n",
    "        'AutoNCP': CfC(AutoNCP(units=100, output_size=2))\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        optimizer = nn.Adam(learning_rate=0.001)\n",
    "        train_losses = []\n",
    "        train_time = time()\n",
    "        \n",
    "        for epoch in range(100):\n",
    "            def loss_fn(model, x, y):\n",
    "                pred = model(x)\n",
    "                return mx.mean((pred - y) ** 2)\n",
    "            \n",
    "            loss, grads = mx.value_and_grad(model, loss_fn)(model, X_train, y_train)\n",
    "            optimizer.update(model, grads)\n",
    "            train_losses.append(float(loss))\n",
    "        \n",
    "        train_time = time() - train_time\n",
    "        \n",
    "        # Evaluate\n",
    "        pred = model(X_test)\n",
    "        test_loss = float(mx.mean((pred - y_test) ** 2))\n",
    "        \n",
    "        # Profile real-time performance\n",
    "        profiler = MLXProfiler(model)\n",
    "        latency_stats = profiler.profile_compute(\n",
    "            batch_size=1,  # Real-time control\n",
    "            seq_length=1,  # Single step\n",
    "            num_runs=1000\n",
    "        )\n",
    "        \n",
    "        results[name] = {\n",
    "            'train_time': train_time,\n",
    "            'train_loss': train_losses,\n",
    "            'test_loss': test_loss,\n",
    "            'latency': latency_stats['time_mean'] * 1000,  # ms\n",
    "            'latency_std': latency_stats['time_std'] * 1000  # ms\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "control_results = benchmark_control()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "for name, result in control_results.items():\n",
    "    plt.plot(result['train_loss'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.bar(control_results.keys(),\n",
    "        [r['test_loss'] for r in control_results.values()])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Control Performance')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.bar(control_results.keys(),\n",
    "        [r['latency'] for r in control_results.values()],\n",
    "        yerr=[r['latency_std'] for r in control_results.values()])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Latency (ms)')\n",
    "plt.title('Real-time Performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Summary\n",
    "\n",
    "Based on our benchmarks:\n",
    "\n",
    "1. **Sequence Prediction**\n",
    "   - NCP performs best for long-term dependencies\n",
    "   - Dense patterns good for short sequences\n",
    "   - AutoNCP balances performance and efficiency\n",
    "\n",
    "2. **Classification**\n",
    "   - Sparse patterns work well\n",
    "   - Memory usage varies significantly\n",
    "   - Training time differences notable\n",
    "\n",
    "3. **Control Tasks**\n",
    "   - Real-time performance critical\n",
    "   - Latency varies by pattern\n",
    "   - Trade-off between accuracy and speed\n",
    "\n",
    "Recommendations:\n",
    "- Use NCP for complex temporal tasks\n",
    "- Consider AutoNCP for balanced performance\n",
    "- Choose sparsity based on task requirements\n",
    "- Monitor real-time performance carefully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
