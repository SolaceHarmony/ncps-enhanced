{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with Liquid Neural Networks\n",
    "\n",
    "This notebook demonstrates how to use liquid neural networks (CfC and LTC) for time series anomaly detection. We'll cover:\n",
    "- Reconstruction-based anomaly detection\n",
    "- Time-aware anomaly scoring\n",
    "- Comparison of CfC and LTC approaches\n",
    "- Real-world application examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ncps.mlx import CfC, LTC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Anomaly Detection Models\n",
    "\n",
    "We'll implement autoencoder-style models using liquid neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LiquidAutoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder using liquid neurons for anomaly detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, cell_type='cfc'):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = CfC if cell_type == 'cfc' else LTC\n",
    "        self.encoder = self.encoder(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            backbone_units=64,\n",
    "            backbone_layers=2,\n",
    "            return_sequences=True\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = self.encoder(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=input_size,\n",
    "            num_layers=2,\n",
    "            backbone_units=64,\n",
    "            backbone_layers=2,\n",
    "            return_sequences=True\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, time_delta=None):\n",
    "        # Encode\n",
    "        encoded = self.encoder(x, time_delta=time_delta)\n",
    "        # Decode\n",
    "        decoded = self.decoder(encoded, time_delta=time_delta)\n",
    "        return decoded\n",
    "    \n",
    "    def compute_anomaly_score(self, x, time_delta=None):\n",
    "        \"\"\"Compute reconstruction error as anomaly score.\"\"\"\n",
    "        reconstructed = self(x, time_delta=time_delta)\n",
    "        # Use time-weighted MSE if time_delta is provided\n",
    "        if time_delta is not None:\n",
    "            weights = 1.0 / (time_delta + 1e-6)  # Avoid division by zero\n",
    "            error = (x - reconstructed) ** 2\n",
    "            return mx.mean(error * weights, axis=-1)\n",
    "        return mx.mean((x - reconstructed) ** 2, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data with Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_anomaly_data(n_samples=1000, seq_length=50, n_features=3, anomaly_ratio=0.1):\n",
    "    \"\"\"Generate synthetic time series with anomalies.\"\"\"\n",
    "    # Generate normal data\n",
    "    t = np.linspace(0, 4*np.pi, seq_length)\n",
    "    normal_data = np.zeros((n_samples, seq_length, n_features))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Generate multiple frequency components\n",
    "        freq = 1.0 + 0.1 * np.random.randn()\n",
    "        phase = 2 * np.pi * np.random.rand()\n",
    "        \n",
    "        for j in range(n_features):\n",
    "            normal_data[i, :, j] = np.sin(freq * t + phase + j*np.pi/3)\n",
    "    \n",
    "    # Add noise\n",
    "    normal_data += 0.1 * np.random.randn(*normal_data.shape)\n",
    "    \n",
    "    # Generate anomalies\n",
    "    n_anomalies = int(n_samples * anomaly_ratio)\n",
    "    anomaly_indices = np.random.choice(n_samples, n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        # Random anomaly type\n",
    "        anomaly_type = np.random.choice(['spike', 'shift', 'trend'])\n",
    "        \n",
    "        if anomaly_type == 'spike':\n",
    "            # Add sudden spikes\n",
    "            spike_idx = np.random.randint(seq_length)\n",
    "            normal_data[idx, spike_idx] += 3 * np.random.randn(n_features)\n",
    "        elif anomaly_type == 'shift':\n",
    "            # Add level shifts\n",
    "            shift_idx = np.random.randint(seq_length)\n",
    "            normal_data[idx, shift_idx:] += 2 * np.random.randn(n_features)\n",
    "        else:\n",
    "            # Add abnormal trends\n",
    "            trend = np.linspace(0, 2, seq_length)[:, None] * np.random.randn(n_features)\n",
    "            normal_data[idx] += trend\n",
    "    \n",
    "    # Create labels\n",
    "    labels = np.zeros(n_samples)\n",
    "    labels[anomaly_indices] = 1\n",
    "    \n",
    "    # Generate variable time steps\n",
    "    time_delta = np.ones((n_samples, seq_length, 1))\n",
    "    for i in range(n_samples):\n",
    "        time_delta[i] += 0.1 * np.random.randn(seq_length, 1)\n",
    "    \n",
    "    return mx.array(normal_data), mx.array(time_delta), labels\n",
    "\n",
    "# Generate data\n",
    "X, time_delta, labels = generate_anomaly_data()\n",
    "\n",
    "# Plot example sequences\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    idx = np.where(labels == i%2)[0][0]\n",
    "    plt.plot(X[idx, :, 0])\n",
    "    plt.title(f\"{'Anomaly' if labels[idx] == 1 else 'Normal'} Sequence\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_anomaly_detector(model, X, time_delta, n_epochs=100, batch_size=32):\n",
    "    \"\"\"Train the anomaly detection model.\"\"\"\n",
    "    optimizer = nn.Adam(learning_rate=0.001)\n",
    "    \n",
    "    def loss_fn(model, x, dt):\n",
    "        reconstructed = model(x, time_delta=dt)\n",
    "        return mx.mean((x - reconstructed) ** 2)\n",
    "    \n",
    "    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "    n_samples = X.shape[0]\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_losses = []\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            batch_X = X[batch_idx]\n",
    "            batch_dt = time_delta[batch_idx]\n",
    "            \n",
    "            loss, grads = loss_and_grad_fn(model, batch_X, batch_dt)\n",
    "            optimizer.update(model, grads)\n",
    "            epoch_losses.append(float(loss))\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train CfC model\n",
    "print(\"Training CfC model...\")\n",
    "cfc_model = LiquidAutoencoder(input_size=3, hidden_size=32, cell_type='cfc')\n",
    "cfc_losses = train_anomaly_detector(cfc_model, X, time_delta)\n",
    "\n",
    "# Train LTC model\n",
    "print(\"\\nTraining LTC model...\")\n",
    "ltc_model = LiquidAutoencoder(input_size=3, hidden_size=32, cell_type='ltc')\n",
    "ltc_losses = train_anomaly_detector(ltc_model, X, time_delta)\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cfc_losses, label='CfC')\n",
    "plt.plot(ltc_losses, label='LTC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_detector(model, X, time_delta, labels, threshold_percentile=95):\n",
    "    \"\"\"Evaluate anomaly detection performance.\"\"\"\n",
    "    # Compute anomaly scores\n",
    "    scores = model.compute_anomaly_score(X, time_delta=time_delta)\n",
    "    scores = mx.mean(scores, axis=1)  # Average over sequence length\n",
    "    \n",
    "    # Determine threshold\n",
    "    threshold = np.percentile(scores, threshold_percentile)\n",
    "    predictions = (scores > threshold).astype(np.int32)\n",
    "    \n",
    "    # Compute metrics\n",
    "    tp = np.sum((predictions == 1) & (labels == 1))\n",
    "    fp = np.sum((predictions == 1) & (labels == 0))\n",
    "    fn = np.sum((predictions == 0) & (labels == 1))\n",
    "    tn = np.sum((predictions == 0) & (labels == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'scores': scores,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "# Evaluate models\n",
    "cfc_results = evaluate_detector(cfc_model, X, time_delta, labels)\n",
    "ltc_results = evaluate_detector(ltc_model, X, time_delta, labels)\n",
    "\n",
    "print(\"CfC Results:\")\n",
    "print(f\"Precision: {cfc_results['precision']:.4f}\")\n",
    "print(f\"Recall: {cfc_results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {cfc_results['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nLTC Results:\")\n",
    "print(f\"Precision: {ltc_results['precision']:.4f}\")\n",
    "print(f\"Recall: {ltc_results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {ltc_results['f1']:.4f}\")\n",
    "\n",
    "# Plot score distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(cfc_results['scores'][labels == 0], bins=50, alpha=0.5, label='Normal')\n",
    "plt.hist(cfc_results['scores'][labels == 1], bins=50, alpha=0.5, label='Anomaly')\n",
    "plt.axvline(cfc_results['threshold'], color='r', linestyle='--', label='Threshold')\n",
    "plt.title('CfC Score Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(ltc_results['scores'][labels == 0], bins=50, alpha=0.5, label='Normal')\n",
    "plt.hist(ltc_results['scores'][labels == 1], bins=50, alpha=0.5, label='Anomaly')\n",
    "plt.axvline(ltc_results['threshold'], color='r', linestyle='--', label='Threshold')\n",
    "plt.title('LTC Score Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Example: Server Metrics\n",
    "\n",
    "Let's apply our models to a real-world scenario using server metrics data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_server_metrics():\n",
    "    \"\"\"Simulate loading server metrics data.\"\"\"\n",
    "    # Generate realistic server metrics\n",
    "    n_samples = 1000\n",
    "    seq_length = 60  # 1 hour of minute-level data\n",
    "    \n",
    "    metrics = {\n",
    "        'cpu_usage': [],\n",
    "        'memory_usage': [],\n",
    "        'network_traffic': [],\n",
    "        'response_time': []\n",
    "    }\n",
    "    \n",
    "    t = np.linspace(0, 2*np.pi, seq_length)\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # CPU usage: Daily pattern with noise\n",
    "        cpu = 50 + 30*np.sin(t) + 5*np.random.randn(seq_length)\n",
    "        \n",
    "        # Memory usage: Gradual increase with periodic cleanup\n",
    "        memory = np.minimum(100, 60 + np.cumsum(0.1*np.random.randn(seq_length)))\n",
    "        \n",
    "        # Network traffic: Spiky pattern\n",
    "        network = 500 + 200*np.sin(t) + 50*np.random.exponential(1, seq_length)\n",
    "        \n",
    "        # Response time: Usually stable with occasional spikes\n",
    "        response = 100 + 10*np.random.randn(seq_length)\n",
    "        \n",
    "        metrics['cpu_usage'].append(cpu)\n",
    "        metrics['memory_usage'].append(memory)\n",
    "        metrics['network_traffic'].append(network)\n",
    "        metrics['response_time'].append(response)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    for k in metrics:\n",
    "        metrics[k] = np.array(metrics[k])\n",
    "    \n",
    "    # Combine metrics\n",
    "    data = np.stack([metrics[k] for k in metrics], axis=-1)\n",
    "    \n",
    "    # Add some anomalies\n",
    "    n_anomalies = int(0.1 * n_samples)\n",
    "    anomaly_indices = np.random.choice(n_samples, n_anomalies, replace=False)\n",
    "    labels = np.zeros(n_samples)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        # Random anomaly patterns\n",
    "        if np.random.rand() < 0.3:\n",
    "            # CPU spike with memory leak\n",
    "            data[idx, :, 0] *= 2.0  # CPU spike\n",
    "            data[idx, :, 1] += np.linspace(0, 40, seq_length)  # Memory leak\n",
    "        elif np.random.rand() < 0.6:\n",
    "            # Network congestion with high response times\n",
    "            data[idx, :, 2] *= 0.2  # Network drop\n",
    "            data[idx, :, 3] *= 5.0  # Response time spike\n",
    "        else:\n",
    "            # General system instability\n",
    "            data[idx] *= (1 + 0.5*np.random.randn(seq_length, 4))\n",
    "        \n",
    "        labels[idx] = 1\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    data_reshaped = data.reshape(-1, 4)\n",
    "    data_normalized = scaler.fit_transform(data_reshaped).reshape(data.shape)\n",
    "    \n",
    "    # Generate time deltas (irregular sampling)\n",
    "    time_delta = np.ones((n_samples, seq_length, 1))\n",
    "    time_delta += 0.1 * np.random.randn(*time_delta.shape)  # Some variance in sampling\n",
    "    \n",
    "    return mx.array(data_normalized), mx.array(time_delta), labels\n",
    "\n",
    "# Load and process server metrics\n",
    "server_data, server_time_delta, server_labels = load_server_metrics()\n",
    "\n",
    "# Train model on server data\n",
    "server_model = LiquidAutoencoder(input_size=4, hidden_size=32, cell_type='cfc')\n",
    "server_losses = train_anomaly_detector(server_model, server_data, server_time_delta)\n",
    "\n",
    "# Evaluate on server data\n",
    "server_results = evaluate_detector(server_model, server_data, server_time_delta, server_labels)\n",
    "\n",
    "print(\"Server Metrics Anomaly Detection Results:\")\n",
    "print(f\"Precision: {server_results['precision']:.4f}\")\n",
    "print(f\"Recall: {server_results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {server_results['f1']:.4f}\")\n",
    "\n",
    "# Plot example anomalies\n",
    "plt.figure(figsize=(15, 10))\n",
    "metrics = ['CPU Usage', 'Memory Usage', 'Network Traffic', 'Response Time']\n",
    "\n",
    "# Find an anomaly example\n",
    "anomaly_idx = np.where(server_labels == 1)[0][0]\n",
    "normal_idx = np.where(server_labels == 0)[0][0]\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.plot(server_data[normal_idx, :, i], label='Normal')\n",
    "    plt.plot(server_data[anomaly_idx, :, i], label='Anomaly')\n",
    "    plt.title(metric)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
