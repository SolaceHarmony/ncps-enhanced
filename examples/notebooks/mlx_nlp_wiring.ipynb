{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Neural Circuit Policies\n",
    "\n",
    "This notebook demonstrates how to use wiring patterns for NLP tasks:\n",
    "- Text classification\n",
    "- Sequence modeling\n",
    "- Attention mechanisms\n",
    "- Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ncps.mlx import CfC, LTC\n",
    "from ncps.mlx.wirings import Wiring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Language Model Wiring\n",
    "\n",
    "Create a wiring pattern for language modeling with attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LanguageWiring(Wiring):\n",
    "    \"\"\"Wiring pattern for language processing.\n",
    "    \n",
    "    Architecture:\n",
    "    - Token embeddings\n",
    "    - Multi-head attention\n",
    "    - Position-wise processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        vocab_size: int,\n",
    "        max_seq_length: int = 512\n",
    "    ):\n",
    "        # Size calculations\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        # Total units needed for Q,K,V projections and output\n",
    "        total_units = hidden_size * 4\n",
    "        super().__init__(total_units)\n",
    "        \n",
    "        # Define component ranges\n",
    "        self.query_range = range(0, hidden_size)\n",
    "        self.key_range = range(hidden_size, hidden_size * 2)\n",
    "        self.value_range = range(hidden_size * 2, hidden_size * 3)\n",
    "        self.output_range = range(hidden_size * 3, hidden_size * 4)\n",
    "        \n",
    "        # Set output dimension\n",
    "        self.set_output_dim(vocab_size)\n",
    "        \n",
    "        # Build connectivity\n",
    "        self._build_attention_connections()\n",
    "        self._build_position_connections()\n",
    "    \n",
    "    def _build_attention_connections(self):\n",
    "        \"\"\"Build multi-head attention connections.\"\"\"\n",
    "        # Connect each query to its corresponding key-value pairs\n",
    "        for head in range(self.num_heads):\n",
    "            q_start = head * self.head_size\n",
    "            q_end = (head + 1) * self.head_size\n",
    "            \n",
    "            k_start = self.key_range.start + head * self.head_size\n",
    "            k_end = self.key_range.start + (head + 1) * self.head_size\n",
    "            \n",
    "            v_start = self.value_range.start + head * self.head_size\n",
    "            v_end = self.value_range.start + (head + 1) * self.head_size\n",
    "            \n",
    "            # Query-Key connections\n",
    "            for q in range(q_start, q_end):\n",
    "                for k in range(k_start, k_end):\n",
    "                    self.add_synapse(q, k, 1)\n",
    "            \n",
    "            # Key-Value connections\n",
    "            for k in range(k_start, k_end):\n",
    "                for v in range(v_start, v_end):\n",
    "                    self.add_synapse(k, v, 1)\n",
    "            \n",
    "            # Value-Output connections\n",
    "            for v in range(v_start, v_end):\n",
    "                for o in self.output_range:\n",
    "                    self.add_synapse(v, o, 1)\n",
    "    \n",
    "    def _build_position_connections(self):\n",
    "        \"\"\"Build position-wise processing connections.\"\"\"\n",
    "        # Add position-wise feed-forward connections\n",
    "        for i in range(self.hidden_size):\n",
    "            # Connect to corresponding output neuron\n",
    "            self.add_synapse(i, self.output_range.start + i, 1)\n",
    "            \n",
    "            # Add skip connections\n",
    "            if i % 2 == 0:  # Every other neuron gets skip connection\n",
    "                self.add_synapse(i, self.output_range.start + i + 1, 1)\n",
    "\n",
    "# Create language model\n",
    "wiring = LanguageWiring(\n",
    "    hidden_size=256,\n",
    "    num_heads=8,\n",
    "    vocab_size=10000\n",
    ")\n",
    "\n",
    "model = CfC(\n",
    "    wiring=wiring,\n",
    "    activation=\"gelu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Classification\n",
    "\n",
    "Train the model for text classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_text_data(n_samples=1000, seq_length=50, vocab_size=1000, n_classes=5):\n",
    "    \"\"\"Generate synthetic text data.\n",
    "    \n",
    "    Returns:\n",
    "    - Token sequences\n",
    "    - Class labels\n",
    "    \"\"\"\n",
    "    X = np.random.randint(0, vocab_size, (n_samples, seq_length))\n",
    "    y = np.zeros((n_samples, n_classes))\n",
    "    \n",
    "    # Generate patterns based on token distributions\n",
    "    for i in range(n_samples):\n",
    "        # Assign class based on token patterns\n",
    "        pattern_type = np.random.randint(n_classes)\n",
    "        y[i, pattern_type] = 1\n",
    "        \n",
    "        if pattern_type == 0:\n",
    "            # Repeated tokens\n",
    "            token = np.random.randint(0, vocab_size//5)\n",
    "            X[i, ::2] = token\n",
    "        elif pattern_type == 1:\n",
    "            # Increasing sequence\n",
    "            start = np.random.randint(0, vocab_size//2)\n",
    "            X[i] = np.minimum(start + np.arange(seq_length), vocab_size-1)\n",
    "        elif pattern_type == 2:\n",
    "            # Alternating high/low\n",
    "            X[i, ::2] = np.random.randint(0, vocab_size//2, seq_length//2 + 1)\n",
    "            X[i, 1::2] = np.random.randint(vocab_size//2, vocab_size, seq_length//2)\n",
    "        else:\n",
    "            # Random with local structure\n",
    "            for j in range(0, seq_length, 5):\n",
    "                token = np.random.randint(0, vocab_size)\n",
    "                X[i, j:j+5] = np.random.normal(token, 2, 5).astype(int) % vocab_size\n",
    "    \n",
    "    # Convert to one-hot\n",
    "    X_onehot = np.zeros((n_samples, seq_length, vocab_size))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(seq_length):\n",
    "            X_onehot[i, j, X[i, j]] = 1\n",
    "    \n",
    "    return mx.array(X_onehot), mx.array(y)\n",
    "\n",
    "# Generate data\n",
    "X_train, y_train = generate_text_data(vocab_size=wiring.vocab_size)\n",
    "X_test, y_test = generate_text_data(n_samples=100, vocab_size=wiring.vocab_size)\n",
    "\n",
    "# Train model\n",
    "optimizer = nn.Adam(learning_rate=0.001)\n",
    "\n",
    "def train_step(model, x, y):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    def loss_fn(model, x, y):\n",
    "        # Get sequence output\n",
    "        pred = model(x)[:, -1]\n",
    "        return mx.mean((pred - y) ** 2)\n",
    "    \n",
    "    loss, grads = nn.value_and_grad(model, loss_fn)(model, x, y)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    loss = train_step(model, X_train, y_train)\n",
    "    losses.append(float(loss))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {float(loss):.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(121)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.subplot(122)\n",
    "predictions = model(X_test)[:, -1]\n",
    "confusion = np.zeros((5, 5))\n",
    "for i in range(len(y_test)):\n",
    "    true_class = mx.argmax(y_test[i])\n",
    "    pred_class = mx.argmax(predictions[i])\n",
    "    confusion[true_class, pred_class] += 1\n",
    "\n",
    "plt.imshow(confusion, cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention Visualization\n",
    "\n",
    "Visualize attention patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_attention(model, text_input):\n",
    "    \"\"\"Visualize attention patterns.\"\"\"\n",
    "    # Get attention weights\n",
    "    attention = model(text_input)\n",
    "    \n",
    "    # Plot attention patterns for each head\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for head in range(model.cell.wiring.num_heads):\n",
    "        plt.subplot(2, 4, head + 1)\n",
    "        \n",
    "        # Get head-specific attention\n",
    "        head_attention = attention[0, :, head * model.cell.wiring.head_size:(head + 1) * model.cell.wiring.head_size]\n",
    "        plt.imshow(head_attention, cmap='viridis')\n",
    "        plt.title(f'Head {head}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Attention Patterns')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention for a sample input\n",
    "sample_input = X_test[0:1]\n",
    "visualize_attention(model, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The language wiring pattern demonstrates several advantages:\n",
    "\n",
    "1. **Attention Mechanism**\n",
    "   - Multi-head attention\n",
    "   - Position-wise processing\n",
    "   - Long-range dependencies\n",
    "\n",
    "2. **Classification Performance**\n",
    "   - Pattern recognition\n",
    "   - Sequence understanding\n",
    "   - Context integration\n",
    "\n",
    "3. **Architecture Benefits**\n",
    "   - Efficient attention\n",
    "   - Position encoding\n",
    "   - Skip connections\n",
    "\n",
    "Key considerations for NLP tasks:\n",
    "- Balance attention heads\n",
    "- Handle sequence lengths\n",
    "- Manage vocabulary size\n",
    "- Efficient computation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
